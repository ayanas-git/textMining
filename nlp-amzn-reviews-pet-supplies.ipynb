{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Notebook Author: Ayana Andrews-Joseph\\\n**Data**: Pet Supplies Reviews from [Amazon product data](http://jmcauley.ucsd.edu/data/amazon/)\\\n**Provider**: Julian McAuley, USCD\\\n**Description**: This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\\\nThis dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs).\\\n**5-core**: Subset of the data in which all users and items have at least 5 reviews.\\\n**Format**: one-review-per-lline in (loose) json.\n\n**Definitions**: \n> **reviewerID** - ID of the reviewer, e.g. A2SUAM1J3GNN3B \\\n> **asin** - ID of the product, e.g. 0000013714 \\\n> **reviewerName** - name of the reviewer \\\n> **helpful** - helpfulness rating of the review, e.g. 2/3 \\\n> **reviewText** - text of the review \\\n> **overall** - rating of the product \\\n> **summary** - summary of the review \\\n> **unixReviewTime** - time of the review (unix time) \\\n> **reviewTime** - time of the review (raw) \n\n**NOTE**: Selected a *“Small” subset for experimentation*, which uses `K-cores`. These data have been reduced to extract the k-core, such that each of the remaining users and items have k reviews each. (McAuley) The concept of a k-core was introduced to study the clustering structure of social networks and to describe the evolution of random graphs.","metadata":{}},{"cell_type":"markdown","source":"# PART I","metadata":{}},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"pip install ijson","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Manipulation & Visualization\nimport os\nimport pandas as pd\nimport numpy as np\nimport ijson\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns # used for plot interactive graph. \nsns.set_style('darkgrid')\n\n# Text Manipulation\nfrom textblob import TextBlob # text processing\nfrom textblob import Blobber\nimport nltk\nnltk.download('all') # Download stopwords list, updated lemmatizer, tokenizers, etc.\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import WhitespaceTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer \n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read in data\nrawDat = pd.read_json(\"../input/amzn-reviews-pet-supplies/Pet_Supplies_5.json\",\n                        lines=True,\n                        orient=\"columns\")\nprint(rawDat.shape)\nrawDat.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = rawDat[[\"helpful\", \"reviewText\", \"overall\", \"summary\", \"asin\"]]\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Output raw dataset","metadata":{}},{"cell_type":"code","source":"# Save to Kaggle for export\nrawDatpub = rawDat.loc[ : , rawDat.columns != 'reviewerName']\nrawDatpub.to_csv('rawData_AmazonPetSupplyReviews.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del rawDatpub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overall Rating Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,7))\nsns.set_theme(style=\"whitegrid\")\nax = sns.countplot(data=df, x=\"overall\", edgecolor='black', linewidth=2, palette=(\"rocket_r\"))\nplt.title(\"Overall Rating Distribution\", size = 17)\nplt.xlabel('Rating')\nplt.ylabel('Frequency [n]')\n\nfor p in ax.patches:\n   ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.75))\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The overall rating distribution is \"left-skewed\", there are more 5-star ratings in comparison to the entire dataset.","metadata":{}},{"cell_type":"markdown","source":"## Frequencies on Product Ids","metadata":{}},{"cell_type":"code","source":"prod_pivot = df.asin.value_counts()\nprod_pivot\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get column integer location","metadata":{}},{"cell_type":"code","source":"df.columns.get_loc('summary')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert comment text to lower() for text processing","metadata":{}},{"cell_type":"code","source":"# Set clean data to lowercase for stopwords preprocessing\ndf['summary'] = df['summary'].str.lower()\ndf['reviewText'] = df['reviewText'].str.lower()\ndf['summary'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove Punctuation","metadata":{}},{"cell_type":"code","source":"import string \n\nnopunc = \"\\n\\r\"+string.punctuation\ndf['summary'] = df['summary'].str.translate(str.maketrans('','',nopunc))\ndf['reviewText'] = df['reviewText'].str.translate(str.maketrans('','',nopunc))\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.reviewText[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply `stopwords` \nAlthough the data set is relatively clean, we can apply stop words for further processing (words that do not count in linguistic analysis). The most common SEO stop words are pronouns, articles, prepositions, and conjunctions.","metadata":{}},{"cell_type":"code","source":"# datacheck for stopwords\nprint(stopwords.words('english'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\nstop_words = set(stopwords.words('english'))\n\n#add words that aren't in the NLTK stopwords list\nadd_stopwords = ['pet', 'pets']\nnew_stopwords_list = stop_words.union(add_stopwords)\n\n#remove words that are in NLTK stopwords list\nremove_stopwords = {'no', 'not', 'didnt'} \nstop = set([word for word in new_stopwords_list if word not in not_stopwords])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply stopwords\ndf['clean_text1'] = df['summary'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndf['clean_text2'] = df['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization and Lemmatization","metadata":{}},{"cell_type":"code","source":"# Init tokenizer and Lemmatizer\nw_token = WhitespaceTokenizer()\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_token.tokenize(text)]\n\ndf['text_lemma_sum'] = df['clean_text1'].apply(lemmatize_text)\ndf['text_lemma_raw'] = df['clean_text2'].apply(lemmatize_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform lemma \"list\" to \"string\" structure - Remove brackets and punctuation from lemmatized list\ndf['text_sum'] = df['text_lemma_sum'].str.join(' ')\ndf['text_raw'] = df['text_lemma_raw'].str.join(' ')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add Sentiment","metadata":{}},{"cell_type":"code","source":"print(\"Lemma str of summary text loc: \", df.columns.get_loc('text_sum'),', ', \"and lemma str of original review text loc: \", df.columns.get_loc('text_raw') )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sentiment of reviewers heading\ndef add_sentiment_to_df_sum(df):\n    sentiment_tuple = []\n    \n    for x in range(0, df.shape[0]):\n        QuantTextBlob = TextBlob(df.iloc[x][9])\n        measures = QuantTextBlob.sentiment\n        sentiment_tuple.append(measures)\n    df['textScore_sum'] = sentiment_tuple\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"add_sentiment_to_df_sum(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sentiment of raw review text\ndef add_sentiment_to_df_raw(df):\n    sentiment_tuple = []\n    \n    for x in range(0, df.shape[0]):\n        QuantTextBlob = TextBlob(df.iloc[x][10])\n        measures = QuantTextBlob.sentiment\n        sentiment_tuple.append(measures)\n    df['textScore_raw'] = sentiment_tuple\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"add_sentiment_to_df_raw(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Noting that polarity and subjectivity vary between using the `summary' field vs. the raw text field. We can take a closer look - plan elasticSearch and visual analytics.","metadata":{}},{"cell_type":"markdown","source":"### Add POLARITY [-1.0,1.0]","metadata":{}},{"cell_type":"code","source":"def add_polarity_to_df_sum(df):\n    polarity_list = []\n    \n    for x in range (0, df.shape[0]):\n        QuantTextBlob = TextBlob(df.iloc[x][9])\n        measures = QuantTextBlob.sentiment.polarity\n        polarity_list.append(measures)\n    df['textPolarity_sum'] = polarity_list\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"add_polarity_to_df_sum(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_polarity_to_df_raw(df):\n    polarity_list = []\n    \n    for x in range (0, df.shape[0]):\n        QuantTextBlob = TextBlob(df.iloc[x][10])\n        measures = QuantTextBlob.sentiment.polarity\n        polarity_list.append(measures)\n    df['textPolarity_raw'] = polarity_list\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"add_polarity_to_df_raw(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getPolarity(score):\n    if score < 0: \n        return 'Negative'\n    elif score == 0:\n        return 'Neutral'\n    else:\n        return 'Positive'\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Polarity_sum'] = df['textPolarity_sum'].apply(getPolarity)\ndf['Polarity_raw'] = df['textPolarity_raw'].apply(getPolarity)\ndf.head(30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get Subjectivity","metadata":{}},{"cell_type":"code","source":"def add_subjectivity_to_df_raw(df):\n    polarity_list = []\n    \n    for x in range (0, df.shape[0]):\n        QuantTextBlob = TextBlob(df.iloc[x][10])\n        measures = QuantTextBlob.sentiment.subjectivity\n        polarity_list.append(measures)\n    df['textSubjectivity_raw'] = polarity_list\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"add_subjectivity_to_df_raw(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_out = df[[\"asin\", \"overall\", \"text_sum\", \"text_raw\", \"textPolarity_sum\", \"textPolarity_raw\", \"Polarity_sum\", \"Polarity_raw\", \"textSubjectivity_raw\"]]\n\ndf_out.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save to Kaggle for export\ndf_out.to_csv('nlpAmazonPetSupplyReviews.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PART II","metadata":{}},{"cell_type":"markdown","source":"### Create combo text filed *(summary + review text)*","metadata":{}},{"cell_type":"code","source":"df['comboText'] = df[['text_sum', 'text_raw']].agg(' '.join, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.comboText[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dedup words","metadata":{}},{"cell_type":"code","source":"from collections import OrderedDict\ndf.comboText = df['comboText'].str.split().apply(lambda x: ' '.join(OrderedDict.fromkeys(x).keys()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.comboText[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get Polarity of combined text","metadata":{}},{"cell_type":"code","source":"df.columns.get_loc('comboText')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_polarity_to_df_combo(df):\n    polarity_list = []\n    \n    for x in range (0, df.shape[0]):\n        QuantTextBlob = TextBlob(df.iloc[x][18])\n        measures = QuantTextBlob.sentiment.polarity\n        polarity_list.append(measures)\n    df['textPolarity_combo'] = polarity_list\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"add_polarity_to_df_combo(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Polarity_combo'] = df['textPolarity_combo'].apply(getPolarity)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_out = df[[\"asin\", \n             \"overall\", \n             \"text_sum\", \n             \"text_raw\", \n             \"comboText\",\n             \"textPolarity_sum\", \n             \"textPolarity_raw\",\n             \"textPolarity_combo\", \n             \"Polarity_sum\", \n             \"Polarity_raw\", \n             \"Polarity_combo\", \n             \"textSubjectivity_raw\"]]\n\ndf_out.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save to Kaggle for export\ndf_out.to_csv('nlpAmazonPetSupplyReviews.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PART III","metadata":{}},{"cell_type":"code","source":"!pip install keybert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using KeyBERT for keywords extraction","metadata":{}},{"cell_type":"code","source":"from keybert import KeyBERT\n\n# One entire document\ndoc = ' '.join(np.unique(df['comboText']))\n\nkw_model = KeyBERT()\nkeywords = kw_model.extract_keywords(doc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kw_model.extract_keywords(doc, keyphrase_ngram_range=(1,2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', \n#                         use_mmr=True, diversity=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keyphrase-vectorizers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keyphrase_vectorizers import KeyphraseTfidfVectorizer\n        \n# Init default vectorizer.\nvectorizer = KeyphraseTfidfVectorizer()\n\n# Print parameters\nprint(vectorizer.get_params())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer.fit(doc.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyphrases = vectorizer.get_feature_names_out()\n\nprint(keyphrases)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}